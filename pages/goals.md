Goals and Values
=============

### Goals
1. Do you want money? If you were offered a button that would send you and every human to ever exist, present and future, to money heaven, with the stipulation that there wouldn't be anything to spend it on, nothing to make you happy, just you and everyone else with infinite dollar bills growing from trees for the rest of eternity, would you press it?
2. Do you want to be happy? If you were offered a button that would send you and every human to ever exist, present and future, to classical heaven, with the stipulation that there wouldn't be anything else there, no money or resources, just you and everyone else in infinite happiness for the rest of eternity, would you press it?

It's obvious that the only reason you want money is for its utility in doing other things like being happy. Presumably you won't use the money to make yourself unhappy, _intentionally at least_. But why then do you want to be happy? This seems harder to think about. We want money to achieve something else, but it doesn't seem like we want to be happy to achieve something else. It seems like a goal that has no further goals. A _Terminal Goal_. Obtaining money, on the other hand, was just a means to achieve that Terminal Goal. The moment money cannot help achieve the Terminal Goal, we _should_ stop caring about it. Money in this case is an _Instrumental Goal_.

We can think about other Instrumental Goals, like obtaining a job, which many people do only in order to get money. If we keep going, we can see that there were even more instrumental goals in this chain. We study for tests, so we can get good grades, so we can get a good job, so we can make money, so we can be happy. But there are usually many more instrumental goals as we start to break higher level goals and move down the chain of goals, and it starts to look like a [tree structure](need_image).

### Is vs Ought
It should be clear that this model accurately captures the difference between _Is_ and _Ought_ statements. _Is_ statements indicate facts about the world: "Money can buy icecream; Eating icecream makes you happy", and only through the lens of a Terminal Goal can we turn the _Is_ statements into their _Ought_ equivalents. These are Instrumental Goals. "I ought to get money (instrumental goal) so I can buy icecream (instrumental goal) so I can eat it (instrumental goal) and be happy (terminal goal)".

Yet this model doesn't capture human thinking or behavior very well in a lot of cases. What's missing is the fact that our brains build this tree out of a mess of neurotransmitters and hormones, which were shaped instincts and enigmatic learning algorithms developed by the [first optimization process that happened to turn up](https://www.lesswrong.com/posts/ZyNak8F6WXjuEbWWc/the-wonder-of-evolution). Not to mention, most of the brain is working on [other](https://en.wikipedia.org/wiki/Motor_cortex) [computation](https://en.wikipedia.org/wiki/Language_processing_in_the_brain) [problems](https://en.wikipedia.org/wiki/Visual_perception). But the fact that we aren't logical all of the time does need more explanation.

### Proxies
From an evolutionary standpoint, reproduction is the Terminal Goal. Why don't humans view an activity like eating food as an Instrumental Goal towards that, like how we view money? Why do some things make us happy and others don't? If evolution only cared about our reproduction, wouldn't it be better to make us happy if and only if we have lots of children, then let the intelligent human figure out the rest? If the human knows they want to reproduce, they can just deduce that they need to eat and have sex in order to accomplish that! 

While this is algorithmically elegant, this didn't happen:
1. Proxies like enjoying eating and sex developed long before there was an intelligence that could act on a high level goal. These proxies exist in all animals. They are still with us.
2. Humans still aren't smart enough for this. Physical pain, for instance, is just a proxy for reducing fitness. When you break your finger, pain tells you to stop using that finger so it can heal. Without this, and lacking a modern understanding of biology, it would be hard to know what the right course of action is.

In fact, we can see that a large amount of human behavior, and desires that produce that behavior, are not well thought out Instrumental Goals that converge on our Terminal Goal.

Physical Pain
Phobias
Hunger

these proxies control us, even in some sense, against our own will
AI wont have this problem.



That is definitely more algorithmically elegant, but doesn't work very well. Theoretically all sorts of pleasures and pains that we experience are redundant. Breaking a finger should hurt for the same reason that starving should hurt. They are both reducing your reproductive odds.

   When we eat tasty food, we are instantly given some sort of feedback about whether that was a good decision or not. When we break a finger, we know there is a problem.

[](ai can't play dig dug)




Yes if the organism is intelligent enough to d, but for most of our evolutionary history, we weren't very smart.

 Evolution can only work with what it had available yesterday. 



### Conflict
But sometimes there are multiple paths to achieving a terminal goal.
These goals can start to conflict. Wouldn't it be better if we could align them []()?


### Mutable Minds
Notice that we can substitute the Terminal Goal of happiness for something else.

### Minds That Are Too Mutable




evolution and desires
Interactions between goals can start to grow complex.



Sam Harris has a similar set of ideas, unfortunately he is confused about a few minor details.

It is possible to choose poor instrumental goals.

Is it possible to choose the wrong terminal goal?



[Prereading](http://www.paulgraham.com/disc.html)


