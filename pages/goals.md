Goals and Values
=============


### Goals
1. Do you want money? If you could send yourself and every human to money heaven, with the stipulation that there wouldn't be anything to spend it on and nothing to make you happy, just you and everyone else with infinite Federal Reserve Notes growing from trees for the rest of eternity, would you do it?
2. Do you want to be happy? If you could send yourself and every human to happiness heaven, with the stipulation that there wouldn't be anything else there, no money or resources, just you and everyone else in infinite happiness for the rest of eternity, would you do it?

It's obvious that the only reason you want money is for its utility in doing other things like being happy. Presumably you won't use the money to make yourself unhappy, _intentionally at least_. But why then do you want to be happy? This seems harder to think about. We want money to achieve something else, but it doesn't seem like we want to be happy to achieve something else. It seems like a goal that has no further goals. A _Terminal Goal_. Obtaining money, on the other hand, is just a means to achieve that Terminal Goal. The moment money cannot help achieve your Terminal Goal, you _should_ stop caring about it. Money in this case is an _Instrumental Goal_.

We can think about other Instrumental Goals, like obtaining a job, which many people do only in order to get money. If we keep going, we can see that there were even more instrumental goals in this chain. We study for tests, so we can get good grades, so we can get a good job, so we can make money, so we can be happy. But there are usually many more instrumental goals as we start to break higher level goals and move down the chain of goals, and it starts to look like a [tree structure](need_image).


### Is vs Ought
It should be clear that this captures the difference between _Is_ and _Ought_ statements. _Is_ statements indicate facts about the world: "Money can buy icecream; Eating icecream makes humans happy", and only through the lens of a Terminal Goal can we turn the _Is_ statements into their _Ought_ equivalents. "If those two statements are true, I ought to get money so I can buy and eat icecream and be happy! <sup>(terminal goal)</sup>". A robot programmed to build paperclips could agree on the _Is_ statements, but disagree on what to do with that information. Icecream doesn't help build paperclips.


### Multiple Goals
The _Terminal and Instrument Goal_ model doesn't capture human thinking or behavior very well in a lot of cases though. I want to eat icecream even when it would make me overweight and unhappy in the long term. What's up with that? As far as evolution is concerned, reproduction is the Terminal Goal. Why did it make me enjoy sugar? Why didn't we evolve to view an activity like eating sweet food or having sex as _just_ an Instrumental Goal towards reproduction, like how we view money? If evolution only cared about our reproduction, wouldn't it be better to make us happy if and only if we have lots of successful children, then let the intelligent brain figure out the rest? If the human knows they want to reproduce, they can just deduce that they need to eat and have sex in order to accomplish that!

While this is algorithmically elegant and simple, this didn't happen. Instead we have a bunch of [heuristics](https://en.wikipedia.org/wiki/Heuristic) about food, sex, and social structure. Why?

1. Anyone who already knows the definition of [heuristic](https://en.wikipedia.org/wiki/Heuristic) should already know, which is that this is an insanely hard computational problem. You have to determine cause and effect between two events that may be very seperated in time and space. State of the art AI only recently managed to tackle [Starcraft II](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii) and [Montezumas Revenge](https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)), where you have to determine cause and effect between two events that might be _a whopping_ 20 minutes apart. Imagine having to figure out the cause and effect of eating food without any feedback. With only one shot at life, your genes can't have you making a logical error like "eating isn't useful" which ends your whole genetic line. So frankly, humans _still_ aren't smart enough for this. Another example, physical pain, is just a heuristic for reducing fitness. When you break your finger, pain tells you to stop using that finger so it can heal. Lacking a modern understanding of biology, there is no way even recent humans would know what to (not) do.
2. Intelligence, with high level goals, evolved _way WAY_ after neurons and brains formed, so the heuristics came first. They developed before there was an intelligence that could act on a high level goal. If humans can't figure out long term payouts, our rodent ancestors certainly couldn't.


So what evolution would ideally have considered an instrumental goal, like eating food, has become a heuristic, hard wired into organisms as a kind of Terminal Goal (as we experience it anyway). Doing this helps take the computational load off of the organism by providing more immediate feedback about "how you're doing". With the reward system in place, if evolution wants us to do something more often, it can then just wire up that activity to the pleasure center in the form of an instict/predisposition. Intelligence comes in as a system where brains can learn new goals in real time. These goals are experienced in the form of desires. Doing heroin definitely doesn't improve your fitness, but it definitely activates your reward center, and heroin users have learned this


Note that in this model, addiction can be viewed purely as a learned goal. Addiction is just a brain learning how to reward itself, and/or possibly avoid pain. The reason most people don't have an eating addiction is because your your body has a way to stop you from eating too much. You'll start to feel sick, or more likely just lose the interest in eating temporarily. It doesn't have a way to get you to stop using heroin, because that's not something animals/ancestors faced as a significant danger, so your genes never evolved a mechanism to stop doing it.


### Problems With Multiple Goals
But there is a significant issue with all these heuristics/multiple goals: they often conflict with eachother. 

Going back to Instrumental Goals, we determined you really only wanted money so you can do other, more desireable activities. You want to have more money so you can buy more icecream. But you also want to eat healthy so you can live longer and have more time to eat that icecream. These are effectively investment problems, (do just enough work time so you can get the most play time) and only some investments are worth it. It makes sense that you don’t spend all your time being productive if you never get to spend time enjoying the outputs of that productivity.

In the modern environment, my desire to "obtain nutrition" _by eating icecream_, "conserve energy" _by sitting on the couch_, and "learn and explore the environment" _by playing videogames_ does not align with my desire to have a fit body that would get me the sexiest dates. In fact, none of those desires align with eachother. Money spent on videogames is money I can't spend on icecream. And any money spent means more time working, which precludes all those activities.

Why don't I do heroin, or other hard drugs, if I know they would be pleasurable? I suspect most people's answers boil down to that it would conflict with something else they are trying to do, like have children, or not be homeless. If you start loving heroin, it's going to be harder to get your other goals accomplished. It should be obvious that conflicts in one's goals (aka desires) isn't enjoyable.


### Aligning Multiple Goals
So imagine we develop the technology to remove conflicting desires. A heroin addict would probably remove that _do-heroin-more-often_ desire, but why stop there? Videogames aren't helping me living longer or get more dates, so let's nix that. Sugar? Definitely don't need that. I could eat Kale all day and look like a greek god without putting in any effort.

But wait, why did I choose to get rid of the sugar desire rather than the look-sexy desire? It seems sort of arbitrary. As long as I remove all but one desire, I shouldn't get any conflicts!


### Problems With Single Goals
But actually, some goals are better than others. If you love making money, you can use that money to help you make _more_ money tomorrow. But if you want to eat icecream, eating icecream doesn't let you eat more icecream tomorrow. 

You're going to have to do other work in order to get that icecream, so wouldn't it be easier if the work was as fun as the icecream? Having desires conflict with themselves is annoying, and having them align is great. Then you just get to have fun the whole time! The activities that are "productive" is a consequence of the laws of the universe, so we can’t realistically change them to match our desireable activities. But with this hypothetical technology we can change our desires to match productive activities. 

### True Alignment
So now imagine if you could get all the benefits of productive activities while only doing your desireable ones. Wouldn’t that be better? Wouldn’t it be **a lot** better if you got to do exactly what you wanted all the time, while reaping all the benefits of purely productive activities? 

So we remove all the desires/heuristics until we have one Terminal Goal. What would it be? What is the single, most self aligned, goal?







