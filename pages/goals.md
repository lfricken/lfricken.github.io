Goals and Values
=============


### Goals
1. Do you want money? If you could send yourself and every human to money heaven, with the stipulation that there wouldn't be anything to spend it on and nothing to make you happy, just you and everyone else with infinite Federal Reserve Notes growing from trees for the rest of eternity, would you do it?
2. Do you want to be happy? If you could send yourself and every human to happiness heaven, with the stipulation that there wouldn't be anything else there, no money or resources, just you and everyone else in infinite happiness for the rest of eternity, would you do it?

It's obvious that the only reason you want money is for its utility in doing other things like being happy. Presumably you won't use the money to make yourself unhappy, _intentionally at least_. But why then do you want to be happy? This seems harder to think about. We want money to achieve something else, but it doesn't seem like we want to be happy to achieve something else. It seems like a goal that has no further goals. A _Terminal Goal_. Obtaining money, on the other hand, is just a means to achieve that Terminal Goal. The moment money cannot help achieve your Terminal Goal, you _should_ stop caring about it. Money in this case is an _Instrumental Goal_.

We can think about other Instrumental Goals, like obtaining a job, which many people do only in order to get money. If we keep going, we can see that there were even more instrumental goals in this chain. We study for tests, so we can get good grades, so we can get a good job, so we can make money, so we can be happy. But there are usually many more instrumental goals as we start to break higher level goals and move down the chain of goals, and it starts to look like a [tree structure](need_image).


### Is vs Ought
It should be clear that this captures the difference between _Is_ and _Ought_ statements. _Is_ statements indicate facts about the world: "Money can buy icecream; Eating icecream makes humans happy", and only through the lens of a Terminal Goal can we turn the _Is_ statements into their _Ought_ equivalents. "If those two statements are true, I ought to get money so I can buy and eat icecream and be happy! <sup>(terminal goal)</sup>". A robot programmed to build paperclips can agree on th


### Multiple Goals
The _Terminal and Instrument Goal_ model doesn't capture human thinking or behavior very well in a lot of cases though. I want to eat icecream even when it would make me overweight and unhappy in the long term. What's up with that? As far as evolution is concerned, reproduction is the Terminal Goal. Why did it make me enjoy sugar? Why didn't we evolve to view an activity like eating sweet food as _just_ an Instrumental Goal towards that, like how we view money? If evolution only cared about our reproduction, wouldn't it be better to make us happy if and only if we have lots of successful children, then let the intelligent brain figure out the rest? If the human knows they want to reproduce, they can just deduce that they need to eat and have sex in order to accomplish that! 

While this is algorithmically elegant, this didn't happen.

1. Any machine learning experts in the audience should know immediately one reason this didn't happen, which is that this is an insanely hard computational problem. You have to determine cause and effect between two events that may be very seperated in time and space. State of the art AI [only recently managed to tackle Starcraft II](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii), where you have to determine cause and effect between two events that might be _a whopping_ 20 minutes apart. Imagine having to figure out the cause and effect of sex, which takes 9 months to "pay off" even if things go well. So frankly, humans _still_ aren't smart enough for this, and we are the most intelligent animal. Another example, physical pain, is just a proxy for reducing fitness. When you break your finger, pain tells you to stop using that finger so it can heal. Lacking a modern understanding of biology, there is no way even recent humans would know what to (not) do. My citation is almost all "medicine" before 1700 (_and that was with the immediate feedback!_)
2. Intelligence evolved _way WAY_ after evolution started working on its optimization process, so the proxies came first anyway. Proxies developed before there was even an intelligence that could act on a high level goal. If humans can't figure out long term payouts, our rodent ancestors certainly couldn't. 


So what evolution would ideally have considered an instrumental goal, like eating food, has effectively been hard wired into organisms as a kind of Terminal Goal (as we experience it anyway). Doing this helps take the computational load off of the organism by providing more immediate feedback about "how you're doing". With the reward system in place, if evolution wants us to do something more often, it can then just wire up that activity to the pleasure center in the form of an instict/predisposition. It has also setup a system where brains can learn new goals in real time. These goals are experienced in the form of desires. Doing heroin definitely doesn't improve your fitness, but it definitely activates your reward center, and so you'll quickly learn to do that more often. 


### Problems With Multiple Goals
There is a really significant issue with having multiple goals: they often conflict with eachother. 

In the modern environment, my desire to "obtain nutrition" _by eating icecream_, "conserve energy" _by sitting on the couch_, and "learn and explore the environment" _by playing videogames_ does not align with my desire to have a fit body that would get me the sexiest dates. In fact, none of those desires even align with eachother. When I spend my time and money eating, that means less videogames, and vice versa.

Why not do heroin, cocaine, or other hard drugs? I guarantee it's more pleasurable than reading this. In fact, I guarantee it's more fun than just about anything you have done. Most people's answers boil down to that it would conflict with something else they are trying to do, like have children, or not be homeless. If you start loving heroin, it's going to be harder to get your other goals accomplished. It should be obvious that conflicts in one's goals (aka desires) isn't enjoyable.


### Alignment
Imagine if you could remove desires at will. Then you could remove all but one desire, and you wouldn't have any conflicts. A heroin addict would probably remove that _do-heroin-more-often_ desire, but why stop there? You have FULL CONTROL. Videogames aren't helping me living longer or get more dates, so let's nix that. Sugar? Definitely don't need that. Eating in general actually. I could eat Kale all day and look like a greek god without putting in any effort.

But wait, why did I choose to get rid of the eating desire rather than the look-sexy desire? It seems sort of arbitrary, as long as I remove all but one desire, I shouldn't get any conflicts.


### Problems With Single Goals
But actually, some goals are better than others. If you love making money, you can use that money to help you make more  money tomorrow. But if you want to eat icecream, eating icecream doesn't let you eat more icecream tomorrow.

You're going to have to do other work in order to get that icecream, so wouldn't it be easier if the work was as fun as the icecream? Then you just get to have fun the whole time! The activities that are "productive" is a consequence of the laws of the universe, so we can’t realistically change them to match our desireable activities. But maybe we change our desires to match productive activities then?


Going back to Instrumental Goals, in what sense do you _want_ to do "work"? Probably so you can do more of those desireable activities. You want to have more money so you can buy more icecream. But you also want to eat healthy so you can live longer and have more time to eat that icecream. These are effectively investment problems, (do just enough work time so you can get the most play time) and only some investments are worth it. It makes sense that you don’t spend all your time being productive if you never get to spend time enjoying the outputs of that productivity.

Having desires conflict is really annoying, and having them align is great.

### True Alignment

But now imagine if you could get all the benefits of productive activities while only doing your desireable ones. Wouldn’t that be better? Wouldn’t it be **a lot** better if you got to do exactly what you wanted all the time, while reaping all the benefits of purely productive activities? 

So, if you obtained full control over your own desires, and could change them at will, what would you do? A

So let's just keep going, removing all the proxies until we're just left with a Terminal Goal, and knowledge of how to achieve it. Now take one more step. Just as goal like playing videogames doesn't align  don't necessarily produce instrumental goals that align, Not all Terminal Goals are made equal either, as some produce Instrumental Goals that conflict. If you want to make paper clips, you'll have to make a paper clip factory. That factory is material and time that wasn't spent making paper clips.

If on the other hand you want to make factories, well that's a lot easier better, because you need factories to make more factories. You'll be able to have fun while working. Given what we know about the universe, what the most aligned Terminal Goal?






-- for another article, pshcopaths

[](ai can't play dig dug)




Yes if the organism is intelligent enough to d, but for most of our evolutionary history, we weren't very smart.

 Evolution can only work with what it had available yesterday. 



### Conflict
But sometimes there are multiple paths to achieving a terminal goal.
These goals can start to conflict. Wouldn't it be better if we could align them []()?


### Mutable Minds
Notice that we can substitute the Terminal Goal of happiness for something else.

### Minds That Are Too Mutable




evolution and desires
Interactions between goals can start to grow complex.



Sam Harris has a similar set of ideas, unfortunately he is confused about a few minor details.

It is possible to choose poor instrumental goals.

Is it possible to choose the wrong terminal goal?



[Prereading](http://www.paulgraham.com/disc.html)


